{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9214609-92fc-4e80-a22c-70f7698ae535",
   "metadata": {},
   "source": [
    "<i>Written by: Ahsan Khan, On behalf of Alberta Machine Intelligence Institute for the Al Pathways Partnership supported by Prairies Economic Development Canada</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d486b609-4985-49a9-bf44-82b3f6692afb",
   "metadata": {},
   "source": [
    "# Lab: Computer Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d6524d-069c-45a8-b72b-2b49517dcc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34dbb64-5d35-422b-b2c0-1cf9ad83c333",
   "metadata": {},
   "source": [
    "Retrieving image from a link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43067d71-a197-4d66-bd05-2af6dbd3618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\n",
    "  'https://media.istockphoto.com/photos/skyline-of-downtown-edmonton-alberta-canada-at-twilight-picture-id1279220699?b=1&k=20&m=1279220699&s=170667a&w=0&h=9ARNx2BH-clzkU8VSqV5yH9Sr_yTJfs9b9_ZS217SCk=',\n",
    "   \"edmonton.png\")\n",
    "  \n",
    "img = Image.open(\"edmonton.png\")\n",
    "#plt.imshow(img)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b053ba-ba9c-415c-a2cf-5180b08822db",
   "metadata": {},
   "source": [
    "## Activity One: Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac6323-64cc-47cf-a51d-a7d16d28ccd0",
   "metadata": {},
   "source": [
    "For this first activity you will be performing some image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff40daf-6cfd-4aa2-a1f7-70c1d39dbec8",
   "metadata": {},
   "source": [
    "##### [A] Convert the image into a numpy array and display the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab1683-53d2-4c02-ae28-84f540be1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(img)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c56dc-77f2-4eaf-9260-f4785c10688e",
   "metadata": {},
   "source": [
    "##### [A] The image has 3 color channels (RGB). Make a copy of the original image and display the new copy image in blue only (red and green channels set to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e3491-94cf-4c5c-8143-34fcf115827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_rgb_image = image.copy()\n",
    "non_rgb_image[:, :, 0] = 0\n",
    "non_rgb_image[:, :, 1] = 0\n",
    "\n",
    "plt.imshow(non_rgb_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cf4e9-ae1f-48d4-9ece-b78360124ece",
   "metadata": {},
   "source": [
    "##### [A] Slice the image to only show the bridge (some part of the bridge can be cut but the displayed image should mostly contain the bridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29bf4a3-f42e-46bb-afcb-bda12f907b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image[170:290,35:175,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4faf7-72af-4dfb-8b3b-49bcb915f4b6",
   "metadata": {},
   "source": [
    "##### [A] Sharpen the image using open cv library (Can use any filter of your choice - read the documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e0d19-b14b-4403-a5c4-67e6d68f159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17d1b0-a9aa-4ef3-a838-d5111f2ffd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.array([[0, -1, 0], [-1, 5,-1], [0, -1, 0]])\n",
    "sharpened_image = cv2.filter2D(image, -1, kernel)\n",
    "plt.imshow(sharpened_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e0a9d",
   "metadata": {},
   "source": [
    "**Interactive HSV Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94361762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ipywidgets import *\n",
    "\n",
    "def f(hMin, hMax, Smin, Smax, Vmin, Vmax):\n",
    "    raw_img = cv2.imread('edmonton.png')\n",
    "    img = cv2.cvtColor(raw_img ,cv2.COLOR_BGR2RGB)  \n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    Lhsv = np.array([hMin, Smin, Vmin])\n",
    "    Uhsv = np.array([hMax, Smax, Vmax])\n",
    "    mask = cv2.inRange(hsv,Lhsv ,Uhsv)\n",
    "    r = cv2.bitwise_and(img, img, mask=mask)\n",
    "    plt.imshow(r)\n",
    "  \n",
    "    \n",
    "interactive(f, hMin=(0,255), hMax=(0,255), Smin=(0,255), Smax=(0,255), Vmin=(0,255), Vmax=(0,255))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb26db-8a82-47a8-b82e-600a2a87d742",
   "metadata": {},
   "source": [
    "##### [A] Normalize the picture of edmonton "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d08899-f7bb-4169-b2d4-45d8ca431d9a",
   "metadata": {},
   "source": [
    "## Activity Two: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc1ae0-f4bb-4065-9d15-c28e77eed8ae",
   "metadata": {},
   "source": [
    "In the previous machine learning course (ML 1) during your last lab you were introduced to the mnist dataset. You were required to classify each number using a logistic regression. This time you will be classifying the numbers using a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "418b3204-2ad1-4472-9a37-9e89b87ec304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe924585-ae4e-40be-9506-741a986e2977",
   "metadata": {},
   "source": [
    "##### [A} Display the very first element (number) in the train_X set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc47fe06-6a08-4f01-8182-b6d78fcd6cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fef29ccbe20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d9166c-469e-4dd2-b850-2b4d1ed9b676",
   "metadata": {},
   "source": [
    "##### [A] Verify the y label for the very first element in the train_y set so it matches with the X element above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf13dee-0d47-407d-b8ee-6f270117fd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4a453-e8e2-47da-9646-5b89086c1521",
   "metadata": {},
   "source": [
    "##### [A] Normalize your images (divide by 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62accfa-896a-4c40-a2c8-2a61086e15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X/255\n",
    "test_X = test_X/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b28d7ee-cfa7-46b3-967c-54b69bafc113",
   "metadata": {},
   "source": [
    "##### [A] Build a neural network architecture aside from the dense layers, include maxpooling, flattening layers, and dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe9d2430-f5a6-45bc-87bd-e02e2a51c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40fc58d",
   "metadata": {},
   "source": [
    "**Visualize your neural network model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c66c73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7d5ab-f98b-4af9-8938-faeb6bc5d322",
   "metadata": {},
   "source": [
    "##### [A] Compile and fit your image data on your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01aaf64-cf48-409f-b6a3-443900eb4ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(x=train_X,y=train_y, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36df303d-ff2d-45da-b6aa-a995f6ec7b04",
   "metadata": {},
   "source": [
    "#### [A] Evaluate your model and make predictions on a few elements (number images) from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364619dc-39ff-48a6-aa18-ea8132a3a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268698e-a70f-411a-a9b5-d07ffb986037",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_X[75].reshape(1, 28, 28, 1))\n",
    "#pred = model.predict(test_X[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41af47-7f12-40b0-b57c-098a09a4c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_X[75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6fb68-0580-4397-8372-b956c9f26cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898092b9-135e-463a-86f8-c044fc87af3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Maybe Activity 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
